%
% references.bib -- Bibliography file for the paper ml
%
% (c) 2020 Autor, Hochschule Rapperswil
%

@online{ml:bibtex,
	title = {BibTeX},
	url = {https://de.wikipedia.org/wiki/BibTeX},
	date = {2020-02-06},
	year = {2020},
	month = {2},
	day = {6}
}
@online{ml:universala-approximator-theorem,
	title = {Universal approximation theorem},
	url = {https://en.wikipedia.org/wiki/Universal_approximation_theorem},
    urldate = {2023-07-15},
	year = {2023},
	month = {7},
	day = {10}
}
@online{ml:computational-complexity-math-op,
	title = {Computational complexity of mathematical operations},
	url = {https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations},
    urldate = {2023-07-15},
	year = {2023},
	month = {6},
	day = {4}
}
@online{ml:computational-complexity-matrix-mult,
	title = {Computational complexity of matrix multiplication},
	url = {https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication},
    urldate = {2023-07-18},
	year = {2023},
	month = {7},
	day = {18}
}
@book{ml:numerical-analysis,
	title = {Numerical Analysis},
	author = {David Kincaid and Ward Cheney},
	publisher = {American Mathematical Society},
	year = {2002},
	isbn = {978-8-8218-4788-6},
	inseries = {Pure and applied undegraduate texts},
	volume = {2}
}

@article{ml:mendezmueller,
        author = { Tabea Méndez and Andreas Müller },
        title = { Noncommutative harmonic analysis and image registration },
        journal = { Appl. Comput. Harmon. Anal.},
        year = 2019,
        volume = 47,
        pages = {607--627},
        url = {https://doi.org/10.1016/j.acha.2017.11.004}
}

@book{ml:introduction-to-ml,
    title = {An Introduction to Machine Learning},
    author = { Gopinath Rebala and Ajay Ravi and Sanjay Churiwala },
    publisher = {Springer Cham},
    year = {2019},
    isbn = {978-3-030-15729-6},
    edition = {1}
}
@book{ml:ml-tom-mitchell,
    title = {Machine Learning},
    author = { Tom M. Mitchell },
    publisher = {McGraw-Hill Science/Engineering/Math},
    year = {1997},
    isbn = {0070428077},
    edition = {1}
}
@InProceedings{ml:pmlr-v97-dao19a,
  title = 	 {Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations},
  author =       {Dao, Tri and Gu, Albert and Eichhorn, Matthew and Rudra, Atri and Re, Christopher},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1517--1527},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/dao19a/dao19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/dao19a.html},
  abstract = 	 {Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural prior they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points—the first time a structured approach has done so—with 4X faster inference speed and 40X fewer parameters.}
}
