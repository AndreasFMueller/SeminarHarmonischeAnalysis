%
% 4-sturmliouville.tex -- Sturm-Liouville-Probleme
%
% (c) 2022 Prof Dr Andreas Müller, OST Ostschweizer Fachhochschule
%
\section{Sturm-Liouville-Problem
\label{buch:orthofkt:section:sturmliouville}}
\kopfrechts{Sturm-Liouville-Problem}
Das Beispiel~\ref{buch:orthfkt:sa:beispiel:D2} zeigt, dass
Eigenfunktionen eines Differentialoperators zu interessanten
Familien orthogonaler Funktionen führen können.
Die daraus abgeleitete harmonische Analysis kann zur Lösung
gewisser Differentialgleichungen genutzt werden, die diesen
Differentialoperator verwenden.
Ziel dieses Abschnitts ist zu zeigen, dass das Beispiel auf
eine wesentlich grössere Klasse von Differentialgleichungen
erweitert werden kann.

%
% Der Sturm-Liouville-Differentialoperator
%
\subsection{Der Sturm-Liouville-Differentialgoperator
\label{buch:orthofkt:sturmliouville:subsection:operator}}
Sei $a<b$ und seien zwei Funktionen
$p(x)\in C^1([a,b])$, $q(x)\in C([a,b])$ gegeben.
Wir definieren den sogenannten {\em Sturm-Liouville-Operator}
\index{Sturm-Liouville-Operator}%
\[
L = \frac{d}{dx} p(x) \frac{d}{dx} + q(x),
\]
der die Funktion $y(x)$ auf
\begin{align*}
(Ly)(x)
&=
\frac{d}{dx}p(x)\frac{d}{dx}y(x) + q(x)y(x)
\\
&=
\frac{d}{dx}p(x)y'(x) + q(x)y(x)
\\
&=
p'(x)y'(x)+p(x)y''(x)+q(x)y(x)
\end{align*}
abbildet.
Die Funktion $q(x)$ wirkt also durch punktweise Multiplikation,
während $p(x)$ und $p'(x)$ als Gewichtsfunktionen der zweiten und
ersten Ableitung von $y(x)$ eingehen.

Ableitungen kommen nur im ersten Teil vor, den wir mit
\[
L_0
=
\frac{d}{dx}p(x)\frac{d}{dx}
\]
bezeichnen.

%
% Der Sturm-Liouville-Operator ist selbstadjungiert
%
\subsection{Der Sturm-Liouville-Operator ist selbstadjungiert}
Wir betrachten die Frage der Selbstadjungiertheit des
Operators $L$ bezüglich des Standardskalarproduktes
\[
\langle f,g\rangle
=
\int_a^b f(x)g(x)\,dx.
\]
Der Multiplikationsoperator mit der Funktion $q(x)$ ist ganz
offensichtlich selbstadjungiert, es gilt nämlich
\[
\langle qf,g\rangle
=
\int_a^b (q(x)f(x))g(x)\,dx
=
\int_a^b f(x)(q(x)g(x))\,dx
=
\langle f,qg\rangle
\]
für alle Funktionen $f$ und $g$, für die die Integrale definiert sind.

Der Operator $L_0$ hat ähnliche Eigenschaften wie die zweite
Ableitung im Beispiel~\ref{buch:orthfkt:sa:beispiel:D2}.
Dazu berechnen wir die Skalarprodukte
\begin{align}
\langle Lf,g\rangle
&=
\int_a^b (Lf)(x)g(x)\,dx
=
\int_a^b \frac{d}{dx}\biggl(q(x)\frac{d}{dx}f(x)\biggr) g(x)\,dx
\\
&=
\biggl[ q(x)f'(x) g(x) \biggr]_a^b
-
\int_a^b q(x)f'(x)g'(x)\,dx
\label{buch:orthfkt:sturmliouville:Lfg}
\\
\langle f,Lg\rangle
&=
\int_a^b f(x) (Lg)(x)\,dx
=
\int_a^b f(x)\frac{d}{dx}\biggl(q(x)\frac{d}{dx}g(x)\biggr)\,dx
\\
&=
\biggl[ f(x) q(x)g'(x)\biggr]_a^b
-
\int_a^b f'(x)q(x)g'(x)\,dx.
\label{buch:orthfkt:sturmliouville:fLg}
\end{align}
Die beiden Integrale stimmen überein, aber der erste Term ist
offensichtlich verschieden.
$L$ ist also im allgemeinen nicht selbstadjungiert.

%
% Randbedingungen
%
\subsection{Randbedingungen}
Aus den Gleichungen
\eqref{buch:orthfkt:sturmliouville:Lfg}
und
\eqref{buch:orthfkt:sturmliouville:fLg}
kann man Bedingungen an die Funktionen ableiten, die überhaupt
zugelassen werden sollen, die den Operator $L_0$ zu einem
selbstadjungierten Operator machen.
Die Funktionen müssen die Bedingungen
\begin{align}
\biggl[ p(x)f'(x) g(x) \biggr]_a^b
&=
\biggl[ p(x)f(x) g'(x) \biggr]_a^b
\notag
\\
\Leftrightarrow\qquad
p(b)f'(b)g(b)
-
p(a)f'(a)g(a) 
&=
p(b)f(a)g'(b)
-
p(a)f(a)g'(a)
\notag
\intertext{oder}
p(b)
\bigl(
f'(b)g(b)-f(b)g'(b)
\bigr)
&=
p(a)
\bigl(
f'(a)g(b)
-
f(a)g'(a)
\bigr)
\notag
\intertext{erfüllen.
Mit der Determinante lässt sich das noch etwas übersichtlicher
als}
\left|\begin{matrix}
p(b)f'(b)&p(b)g'(b) \\
f(b) &g(b)
\end{matrix}\right|
&=
\left|\begin{matrix}
p(a)f'(a)&p(a)g'(a) \\
f(a) &g(a)
\end{matrix}\right|
\label{buch:orthfkt:sturmliouville:det}
\end{align}
schreiben.

Randbedingungen können immer nur für einen Randpunkt gefordert
werden, wir müssen also den Wert der Determinante festlegen.
Angenommen, wir fordern, dass die Determinante einen Wert
$d\ne 0$ annimmt.
Setzen wir die Funktione $s f$ und $g$ ein,
entsteht die Determinante
\[
\left|\begin{matrix}
s p(a)f'(a)&p(a)g'(a) \\
s f(a) &g(a)
\end{matrix}\right|
=
s
\left|\begin{matrix}
p(a)f'(a)&p(a)g'(a) \\
f(a) &g(a)
\end{matrix}\right|
=
s d \ne d
\]
für $s \ne 1$.
Eine solche Forderung führt also auf eine Menge von zulässigen Funktionen,
die nicht mehr ein Vektorraum ist.
Es bleibt daher nur die Möglichkeit, dass die Determinante $=0$ sein muss.

Die Determinante 
\eqref{buch:orthfkt:sturmliouville:det}
ist genau dann Null, wenn die Spalten der Matrix linear abhängig sind.
Insbesondere spannen sie die gleiche Gerade durch den Nullpunkt
in $\mathbb{R}^2$ auf.
In der Ebene kann eine solche Gerade auch durch ihre Normale festgelegt
werden.
Es gibt daher Vektoren
\[
\begin{pmatrix}
h_a\\
k_a
\end{pmatrix}
\qquad\text{und}\qquad
\begin{pmatrix}
h_b\\
k_b
\end{pmatrix},
\]
die auf den Vektoren
\[
\begin{pmatrix}
p(a)f'(a)\\
f(a)
\end{pmatrix}
\qquad\text{und}\qquad
\begin{pmatrix}
p(a)g'(a)\\
g(a)
\end{pmatrix}
\]
senkrecht stehen.
Dies ist gleichbedeutend mit der Randbedingung
\begin{align*}
k_af(a) + h_ap(a)f'(a)&=0
\\
k_bf(b) + h_bp(b)f'(b)&=0.
\end{align*}
Wir fassen die Resultate im folgenden Satz zusammen.

\begin{satz}
Seien $h_a, h_b, k_a, k_b\in \mathbb{R}$ gegeben.
Dann ist der Operator $L_0$ ist selbstadjungiert im Prähilbertraum
\begin{equation}
H(h_a,h_b,k_a,k_b)
=
\left\{
f \in C^2([a,b])
\;
\left|
\;
\begin{aligned}
&\int_a^b |f(x)|^2\,dx < \infty,
\int_a^b |p(x)| |f'(x)|^2\,dx < \infty,
\\
&
\begin{aligned}
k_af(a) + h_ap(a)f'(a) &= 0 \\
k_af(b) + h_bp(b)f'(b) &= 0 
\end{aligned}
\end{aligned}
\right.
\right\}
\label{buch:orthfkt:sturmliouvill:ph}
\end{equation}
mit dem Skalarprodukt
\[
\langle f,g\rangle = \int_a^b f(x)g(x)\,dx.
\]
\end{satz}

%
% Das verallgemeinerte Eigenwertproblem
%
\subsection{Das Eigenwertproblem}
Zu Zahlen $h_a$, $h_b$, $k_a$, $k_b$ ist der Operator $L_0$ im 
Prählibertraum~\eqref{buch:orthfkt:sturmliouvill:ph}
selbstadjungiert.
Wir erwarten, dass die Eigenvektoren dieses Differentialoperators eine
Hilbert-Basis für die Analyse und Synthese von Funktionen in diesem
Prählibertraum liefern.

%
% Das Eigenwertproblem für den Operator $L$
%
\subsubsection{Das Eigenwertproblem für den Operator $L$}
Wir betrachten jetzt den Operator
\[
L
=
\frac{d}{dx}p(x)\frac{d}{dx} + q(x)
=
L_0 + q(x).
\]
Es wurde bereits gezeigt, dass $q(x)$ immer selbstadjungiert ist,
und dass $L_0$ in einem Prähilbertraum der
Form~\eqref{buch:orthfkt:sturmliouvill:ph}
selbstadjungiert wird.
Eine Funktion $y\in H(h_a,h_b,k_a,k_b)$ ist eine Eigenfunktion des
Operators $L$ zum Eigenwert $\lambda$, wenn sie die Differentialgleichung
zweiter Ordnung
\begin{align*}
\frac{d}{dx} p(x) y'(x) + q(x)y(x) = \lambda y(x)
\intertext{oder}
 p(x) y''(x) + p'(x)y'(x) + q(x)y(x) = \lambda y(x)
\end{align*}
erfüllt.
Für $H(h_a,h_b,k_a,k_b)$ kann,
möglicherweise unter zusätzlichen Annahmen über die Funktionen
$p(x)$ und $q(x)$ eine Hilbert-Basis aus Eigenfunktionen
gefunden werden.

%
% Das verallgemeinerte Eigenwertproblem für symmetrische Matrizen
%
\subsubsection{Das verallgemeinerte Eigenwertproblem für symmetrische Matrizen}
Das Eigenwertproblem für eine symmetrische Matrix $A$ sucht einen
Vektor $v\ne 0$ und eine Zahl $\lambda$ derart, dass $Av=\lambda v$.
Es ist aus der elementaren linearen Algebra bekannt, dass eine
symmetrische Matrix mit einer orthogonalen Matrix diagonalisiert werden
kann, es existiert also eine Basis aus orthonormierten Eigenvektoren.

Aus der linearen Algebra ist auch bekannt, dass eine zweite symmetrische
Matrix $B$ gleich mitdiagonalisiert werden kann.
Dazu wird das verallgemeinerte Skalarprodukt $\langle\;\,,\;\rangle_B$ 
verwendet, welches die Werte $\langle u,v\rangle_B=\transpose{u}Bv$ hat.
Der Beweis dieser Tatsache verwendet das Skalarprodukt
$\langle\;\,,\;\rangle_B$ an Stelle des Standardskalarproduktes
$\transpose{u}v$.
Damit dies funktioniert, muss für alle $u,v$
\[
\langle Au,v\rangle_B
=
\langle u,Av\rangle_B
\]
sein.
Dies bedeutet
\[
\left.
\begin{aligned}
\langle Au,v\rangle_B
&=
\transpose{(Au)}Bv
=
\transpose{u}\transpose{A}Bv
=
\transpose{u}ABv
\\
\langle u,Av\rangle_B
&=
\transpose{u}BAv
\end{aligned}
\quad
\right\}
\qquad
\Rightarrow
\quad
AB=BA,
\]
die beiden Matrizen müssen also vertauschen.
Da immer noch gilt, dass die Eigenvektoren von $A$ orthogonal bezüglich
des Standardskalarproduktes sind, ist die Diagonalisierung immer noch
mit einer orthogonalen Matrix möglich.

Das gewöhnliche Eigenwertproblem sucht nach Vektoren, die nur gestreckt
werden.
Eine interessante Verallgemeinerung ist die Frage nach Vektoren, die
die Richtung nicht unbedingt behalten, sondern sie so ändern, wie eine
andere Matrix vorgibt.
Sind $A$ und $B$ $n\times n$-Matrizen, dann ist $v\ne 0$ ein
{\em verallgemeinerter}
Eigenvektor zum Eigenwert $\lambda$, wenn $Av=\lambda Bv$ ist.
Die Matrix $A$ heisst verallgemeinert diagonalisierbar, wenn es
eine Basis aus verallgemeinerten Eigenvektoren von $A$ und $B$ gibt.

Ein Vektor im Nullraum von $A$ ist ein verallgemeinerter Eigenvektor
zum Eigenwert $0$.
Ein Vektor im Nullraum von $B$ erfüllt $Bv=0$.
Er kann daher nur dann ein verallgemeinerter Eigenvektor sein,
wenn auch $Av=0$ ist.

Wenn die Matrix $B$ zusätzlich invertierbar ist, dann erfüllt ein
verallgemeinerter Eigenvektor $B$ die einfachere Gleichung
$B^{-1}Av=\lambda v$, er ist also ein gewöhnlicher Eigenvektor von $B^{-1}A$.
Selbst wenn die Matrizen $A$ und $B$ symmetrisch sind, ist nicht garantiert,
dass $B^{-1}A$ symmetrisch ist.
Das bekannte Resultat, dass symmetrische Matrizen diagonalisierbar sind,
kann also nicht garantieren, dass $A$ und $B$ verallgemeinert diagonalisierbar
sind.
Nur wenn $A$ und $B$ vertauschen, ist
\[
\transpose{(B^{-1}A)}
=
\transpose{A}\transpose{B}^{-1}
=
AB^{-1}.
\]
Symmetrie ist also nur gegeben, wenn die Matrizen $A$ und $B$
vertauschen, was wir im Allgemeinen nicht voraussetzen wollen.

Eine etwas stärkere Bedingung als Invertierbarkeit der Matrix $B$
ist, dass $B$ positiv definit ist.
Dies bedeutet, dass $\transpose{v} Bv > 0$ ist für alle Vektoren
$v\ne 0$.
Daraus folgt, dass $B$ invertierbar ist.
Wäre nämlich $v\ne 0$ ein Vektor mit $Bv=0$, dann wäre auch 
$\transpose{v} Bv=\transpose{v}0=0$.
Unter dieser Voraussetzung kann eine Diagonalisierung gefunden
werden:

\begin{satz}
Sind $A$ und $B$ symmetrische Matrizen mit $B$ positiv definit und ist
$v\ne 0$ ein Vektor, der  ein stationärer Punkt von
\[
f(v) = \frac{\transpose{v}Av}{\transpose{v}Bv}
\]
ist, dann ist $v$ ein verallgemeinerter Eigenvektor.
\end{satz}

Man beachte, dass nicht verlangt wird, dass die beiden Matrizen $A$
und $B$ vertauschen.

\begin{proof}[Beweis]
Sei $u$ ein beliebiger anderer Vektor und $\lambda = f(v)$.
Da $f(v)$ ein stationärer Punkt ist, muss die Ableitung der Funktion 
$t\mapsto f(v+tu)$ verschwinden.
Für eine Matrix $C$ ist
\begin{align*}
\frac{d}{dt}
\transpose{(v+tu)}C(v+tu)
&=
\frac{d}{dt}\bigl(
\transpose{v}Cv
+
t(\transpose{u}Cv+\transpose{v}Cu)
+
t^2 \transpose{u}Cu
\bigr)
=
(\transpose{u}Cv+\transpose{v}Cu)
+
2t \transpose{u}Cu
\\
\Rightarrow\quad
\frac{d}{dt}
\transpose{(v+tu)}C(v+tu)
\bigg|_{t=0}
&=
\transpose{u}Cv+ \transpose{v}Cu
=
\transpose{u}Cv+ \transpose{u}\transpose{C}v
=
2\transpose{u}Cv
.
\end{align*}
Angewendet auf die Funktion $f(v)$ folgt
\[
0
=
\frac{d}{dt}f(v+tu)\bigg|_{t=0}
=
\frac{
\transpose{u}Av \cdot \transpose{v}Bv - \transpose{v}Av\cdot\transpose{u}Bv
}{
(\transpose{v}Bv)^2
}
)
=
\frac{1}{\transpose{v}Bv}
\bigl(
\transpose{u}Au - f(v)\transpose{u}Bu
\bigr)
=
\frac{1}{\transpose{v}Bv}
\bigl(
\transpose{u}Av - \lambda\transpose{u}Bv
\bigr)
\]
und daraus
\[
\transpose{u}(Av-\lambda  Bv)
=
0
\]
für beliebige Vektoren $u$.
Das ist aber genau dann möglich, wenn 
\[
Av-\lambda Bv = 0
\qquad
\Rightarrow
\qquad
Av = \lambda Bv,
\]
und somit genau dann, wenn $v$ ein verallgemeinerter Eigenvektor von 
$A$ und $B$ ist.
\end{proof}

\begin{satz}
Sind $A$ und $B$ symmetrische Matrizen, $B$ positiv definit und $v_1,v_2$
verallgemeinerte Eigenvektoren zu verschiedenen Eigenwerten $\lambda_i$,
$i=1,2$, dann sind die Vektoren $v_i$ bezüglich des Skalarproduktes
$\langle\;\,,\;\rangle_B$ orthogonal: $\langle v_1,v_2\rangle_B=0$.
\end{satz}

\begin{proof}[Beweis]
Ähnlich wie bei der entsprechenden Resultaten für Eigenvektoren
symmetrischer Matrizen, berechnen wir das Skalarprodukt $\langle u,Av\rangle$
auf zwei verschiedene Arten.
\[
\renewcommand{\arraycolsep}{2pt}
\left.
\begin{array}{rclclcl}
\langle v_1,Av_2\rangle
&=&
\transpose{v}_1Av_2
&=&
\transpose{v}_1\lambda_2 Bv_2
&=&
\lambda_2
\langle v_1,Bv_2 \rangle
\\
\mathstrut=
\langle Av_1,v_2\rangle
&=&
\transpose{(\lambda_1Bv_1)}v_2
&=&
\lambda_1
\langle Bv_1,v_2\rangle
&=&
\lambda_1
\langle v_1,Bv_2\rangle
\end{array}
\;\right\}
\quad\Rightarrow\quad
0
=
(\underbrace{\lambda_2-\lambda_1}_{\displaystyle \ne 0})
\langle v_1,Bv_2\rangle.
\]
Da die Eigenwerte verschieden sind, muss
$\langle v_1,Bv_2\rangle=\langle v_1,v_2\rangle_B=0$
sein,
was bedeutet, dass die Vektoren $v_1$ und $v_2$ bezüglich des
verallgemeinerten Skalarproduktes $\langle \;\,,\;\rangle_B$ 
orthogonal sind.
\end{proof}

\begin{satz}
Sind $A$ und $B$ symmetrische Matrizen, $B$ positiv definit, dann 
ist $L=B^{-1}A$ bezüglich des Skalarproduktes $\langle\;\,,\;\rangle_B$
selbstadjungiert.
\end{satz}

\begin{proof}[Beweis]
Da $A$ und $B$ symmetrisch sind, kann man nachrechnen, dass
\[
\renewcommand{\arraycolsep}{2pt}
\begin{array}{rclclcl}
\langle Lu,v\rangle_B
&=&
\langle B^{-1}Au,Bv\rangle
&=&
\langle BB^{-1}Au,v\rangle
&=&
\langle Au,v\rangle
\\
\text{und}\qquad
\langle u,Lv\rangle_B
&=&
\langle u,B(B^{-1}A)v\rangle
&=&
\langle u,Av\rangle
&=&
\langle Au,v\rangle
\end{array}
\]
gleich sind.
Damit ist $L$ selbstadjungiert.
\end{proof}

\begin{satz}
Seien $A$ und $B$ symmetrische Matrizen, $B$ positiv definit, dann gibt
es eine bezüglich des Skalarproduktes $\langle\;\,,\;\rangle_B$
orthonormierte Basis aus verallgemeinerten Eigenvektoren von $A$ und $B$.
\end{satz}

\begin{proof}[Beweis]
In einer beliebigen bezüglich $\langle\;\,,\;\rangle_B$ orthonormierten
Basis wird $B$ zur Einheitsmatrix und $A$ und $L=B^{-1}A$ zu einer
symmetrischen Matrix.
Damit ist das Problem zurückgeführt auf das klassische Problem der
Diagonalisierung einer symmetrischen Matrix.
\end{proof}

%
% Das verallgemeinerte Sturm-Liouville-Eigenwertproblem
%
\subsubsection{Das verallgemeinerte Sturm-Liouville-Eigenwertproblem}
Das verallgemeinerte Sturm-Liouville-Eigenwertproblem ist die Aufgabe,
\index{Sturm-Liouville-Eigenwertproblem!verallgemeinert}%
eine Lösung $y(x)$ der Differentialgleichung
\[
\frac{d}{dx}p(x)\frac{d}{dx} y(x)
+ q(x)y(x)
=
\lambda w(x) y(x)
\]
zu finden.
Das Problem unterscheidet sich von dem vorher untersuchten durch
den zusätzlichen Faktor $w(x)>0$, der auch hier Gewichtsfunktion heisst.
Im verallgemeinerten Eigenwertproblem für Matrizen entspricht $w(x)$ 
der Matrix $B$ und $A$ dem Sturm-Liouville-Operator.
Auf Grund dieser Analogie erwarten wir, dass der wie folgt
definierte Operator $L_w$ bezüglich des Skalarproduktes mit Gewichtsfunktion
$w(x)$ selbstadjungiert ist.

\begin{definition}
Gegeben ist $p(x)\in C^1([a,b])$, $q(x),w(x)\in C([a,b])$ und $w(x)>0$
in $(a,b)$.
Der {\em allgemeine Sturm-Liouville-Operator} ist
\[
L_w
=
\frac{1}{w(x)}
\biggl(
\frac{d}{dx}p(x)\frac{d}{dx}
+
q(x)
\biggr).
\]
Das {\em verallgemeinerte Sturm-Liouville-Eigenwertproblem} ist die Aufgabe,
eine Lösung $y(x)$ der Gleichung
\(
L_wy(x) = \lambda y(x)
\)
zu finden.
\end{definition}

Das verallgemeinerte Eigenwertproblem für $L$ und $w(x)$ kann auf
deas gewöhnliche Eigenwertproblem für den Operator $L_w$ 
zurückgeführt werden, indem das Skalarprodukt mit der Gewichtsfunktion
$w(x)$ verwendet wird.
Dies ist der Inhalt des folgenden Satzes.

\begin{satz}
Der Operator $L_w$ ist selbstadjungiert im Prähilbertraum
\[
H(h_a,h_b,k_a,k_b,w)
=
\left\{
f \in C^2([a,b])
\;
\left|
\;
\begin{aligned}
&\int_a^b |f(x)|^2w(x)\,dx < \infty,
\int_a^b |p(x)|\, |f'(x)|^2w(x)\,dx < \infty,
\\
&
\begin{aligned}
k_af(a) + h_ap(a)f'(a) &= 0 \\
k_af(b) + h_bp(b)f'(b) &= 0 
\end{aligned}
\end{aligned}
\right.
\right\}
\]
mit dem Skalarprodukt $\langle \;\,,\;\rangle_w$ mit der Gewichtsfunktion
$w(x)$.
\end{satz}

\begin{proof}[Beweis]
Wir müssen nachrechnen, dass der Operator $L_w$ selbstadjungiert ist.
Dazu seien Funktionen $f,g\in H(h_a,h_b,k_a,k_b,w)$, wir berechnen die
Skalarprodukte
\begin{align*}
\langle L_wf,g\rangle_w
&=
\int_a^b (L_wf)(x)g(x)w(x)\,dx
=
\int_a^b
\biggl(
\frac{d}{dx}p(x)\frac{d}{dx}f(x) + 
q(x) f(x)
\biggr) g(x)\,dx
=
\langle L_0f,g\rangle + \langle qf,g\rangle
\\
\langle f,L_wg\rangle_w
&=
\int_a^b f(x)(L_wg)(x)w(x)\,dx
=
\int_a^b
f(x)
\biggl(
\frac{d}{dx}p(x)\frac{d}{dx}g(x) + 
q(x) g(x)
\biggr) \,dx
=
\langle f,L_0g\rangle + \langle f,qg\rangle.
\end{align*}
Da der Operator $L_0$ und der Multiplikationsoperator mit $q(x)$ bezüglich
des Standardskalarproduktes $\langle\;\,,\;\rangle$ selbstadjungiert sind,
sind die beiden Ausdrücke auf der rechten Seite gleich.
Damit ist gezeigt, dass $L_w$ bezüglich $\langle\;\,,\;\rangle_w$
selbstadjungiert ist.
\end{proof}
